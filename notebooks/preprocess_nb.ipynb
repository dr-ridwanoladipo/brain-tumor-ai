{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12603451,"sourceType":"datasetVersion","datasetId":7960841}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† **Clinical-Grade Multi-Site Brain Tumor Segmentation with AI**\n---\n<span style=\"color:red\">*by **Ridwan Oladipo, MD | AI Specialist***</span>  \n\nA **state-of-the-art, radiologist-grade nnU-Net v2 pipeline** for automated segmentation of **Whole Tumor (WT)**, **Tumor Core (TC)**, and **Enhancing Tumor (ET)** from multi-modal MRI, targeting **WT Dice ‚â• 90%** and **BraTS Avg ‚â• 80%**.\n\nBuilt to **harmonize multi-institutional MRI data**, preserve tumor anatomy with high fidelity, and empower **neurosurgeons, oncologists, and radiologists** in treatment planning, surgical navigation, and disease monitoring.\n\n---\n\n## **üî¨ Project Scope**\n- üß† **Multi-site MRI harmonization** (N4 bias correction + z-score normalization)  \n- üìè **Dataset fingerprinting**: spacings, shapes, intensity stats, tumor volumes  \n- ‚öôÔ∏è **Optimized brain extraction** (47.5% coverage) with tumor preservation  \n- ü§ñ Professional nnU-Net v2 preprocessing + clinical metrics dashboard  \n- üìä **BraTS-standard evaluation metrics** & automated tumor volume quantification  \n- üöÄ Deployment-ready: **FastAPI + ONNX** inference on AWS ECS  \n\n---\n\n## **üìÇ Dataset Summary**\n- ~750 3D MRI volumes (FLAIR, T1w, T1Gd, T2w)  \n- Glioma types: High-Grade (HGG) + Low-Grade (LGG)  \n- Labels: **0 = background, 1 = edema, 2 = non-enhancing tumor, 3 = enhancing tumor**  \n\n---\n\n## **üí° Why This Matters**  \nAutomated segmentation accelerates precision oncology ‚Äî transforming hours of manual delineation into **seconds of AI-powered analysis**, enabling accurate tumor tracking, improved surgical planning, and early recurrence detection.\n\n---\n> ‚öïÔ∏è **Created by a medical doctor + AI expert, <span style=\"color:red\"><b>Ridwan Oladipo</b></span>, merging clinical neuroimaging insight with cutting-edge deep learning to advance brain tumor care.**","metadata":{}},{"cell_type":"markdown","source":"# üöÄ Ultra-Optimized Brain Tumor AI Environment Setup","metadata":{}},{"cell_type":"markdown","source":"## üß© Core & Medical Imaging Libraries","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport json\nimport nibabel as nib\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# Medical imaging libraries\nimport SimpleITK as sitk\nimport pydicom\nfrom scipy import ndimage\nfrom skimage import measure\nfrom sklearn.model_selection import KFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T12:48:32.077535Z","iopub.execute_input":"2025-08-14T12:48:32.078078Z","iopub.status.idle":"2025-08-14T12:48:37.552039Z","shell.execute_reply.started":"2025-08-14T12:48:32.078042Z","shell.execute_reply":"2025-08-14T12:48:37.550970Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## üöÄ Environment & Dataset Configuration","metadata":{}},{"cell_type":"code","source":"# Visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Dataset configuration\nUSE_FULL_DATASET = False  # Set True for full training\nMAX_VOLUMES = 4 if not USE_FULL_DATASET else None\nRANDOM_SEED = 42\n\n# Create directories\nbase_dir = Path('/kaggle/working/brain_tumor_segmentation')\ndata_dir = Path('/kaggle/input/braintumor')\nprocessed_dir = base_dir / 'processed'\nnnunet_dir = base_dir / 'nnUNet_raw' / 'Dataset001_BrainTumor'\ndicom_dir = base_dir / 'dicom_converted'\nresults_dir = base_dir / 'results'\nvisualization_dir = base_dir / 'visualizations'\n\nfor directory in [base_dir, processed_dir, nnunet_dir, nnunet_dir / 'imagesTr',\n                  nnunet_dir / 'labelsTr', nnunet_dir / 'imagesTs', dicom_dir, results_dir, visualization_dir]:\n    directory.mkdir(exist_ok=True, parents=True)\n\nprint(f\"‚úÖ Environment setup - Using {'FULL' if USE_FULL_DATASET else MAX_VOLUMES} volumes\")\nprint(f\"üéØ Target metrics: WT Dice ‚â• 90%, BraTS Avg ‚â• 80%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T12:51:12.221909Z","iopub.execute_input":"2025-08-14T12:51:12.222556Z","iopub.status.idle":"2025-08-14T12:51:12.236377Z","shell.execute_reply.started":"2025-08-14T12:51:12.222517Z","shell.execute_reply":"2025-08-14T12:51:12.234982Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Environment setup - Using 4 volumes\nüéØ Target metrics: WT Dice ‚â• 90%, BraTS Avg ‚â• 80%\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# üìä Dataset Metadata & DICOM Support","metadata":{}},{"cell_type":"markdown","source":"## üìÑ Load Dataset Metadata and NIfTI Files","metadata":{}},{"cell_type":"code","source":"# Load dataset metadata\nwith open(data_dir / 'dataset.json', 'r') as f:\n    dataset_metadata = json.load(f)\n\nprint(f\"üìä DATASET INFORMATION:\")\nprint(f\"Name: {dataset_metadata['name']}\")\nprint(f\"Description: {dataset_metadata['description']}\")\nprint(f\"Training cases: {dataset_metadata['numTraining']}\")\nprint(f\"Modalities: {', '.join([f'{k}: {v}' for k, v in dataset_metadata['modality'].items()])}\")\nprint(f\"Labels: {', '.join([f'{k}: {v}' for k, v in dataset_metadata['labels'].items()])}\")\n\n# Collect files - filter hidden files\ntrain_image_files = sorted([f for f in (data_dir / 'imagesTr').glob('*.nii*') if not f.name.startswith('._')])\ntrain_label_files = sorted([f for f in (data_dir / 'labelsTr').glob('*.nii*') if not f.name.startswith('._')])\ntest_image_files = sorted([f for f in (data_dir / 'imagesTs').glob('*.nii*') if not f.name.startswith('._')])\n\nprint(f\"Found {len(train_image_files)} training images\")\nprint(f\"Found {len(train_label_files)} training labels\")\nprint(f\"Found {len(test_image_files)} test images\")\n\n# Apply volume limitation for resource constraints\nif not USE_FULL_DATASET and MAX_VOLUMES:\n    np.random.seed(RANDOM_SEED)\n    indices = np.random.choice(len(train_image_files), min(MAX_VOLUMES, len(train_image_files)), replace=False)\n    train_image_files = [train_image_files[i] for i in sorted(indices)]\n    train_label_files = [train_label_files[i] for i in sorted(indices)]\n    print(f\"Limited to {len(train_image_files)} volumes for current run\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T12:51:56.513175Z","iopub.execute_input":"2025-08-14T12:51:56.513783Z","iopub.status.idle":"2025-08-14T12:51:56.691619Z","shell.execute_reply.started":"2025-08-14T12:51:56.513753Z","shell.execute_reply":"2025-08-14T12:51:56.690936Z"}},"outputs":[{"name":"stdout","text":"üìä DATASET INFORMATION:\nName: BRATS\nDescription: Gliomas segmentation tumour and oedema in on brain images\nTraining cases: 484\nModalities: 0: FLAIR, 1: T1w, 2: t1gd, 3: T2w\nLabels: 0: background, 1: edema, 2: non-enhancing tumor, 3: enhancing tumour\nFound 484 training images\nFound 484 training labels\nFound 266 test images\nLimited to 4 volumes for current run\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üè• DICOM Discovery & Conversion","metadata":{}},{"cell_type":"code","source":"print(\"üè• DICOM SUPPORT:\")\n\n# Search for DICOM directories recursively\nall_dicom_dirs = []\nfor root, dirs, files in os.walk(data_dir):\n    for file in files:\n        if file.lower().endswith(('.dcm', '.dicom')) or not '.' in file:\n            dicom_path = Path(root)\n            if dicom_path not in all_dicom_dirs:\n                all_dicom_dirs.append(dicom_path)\n\n# Check for standard DICOM directory structures\ndicom_patterns = ['**/DICOM*', '**/dicom*', '**/DCM*', '**/dcm*']\nfor pattern in dicom_patterns:\n    found_dirs = list(data_dir.glob(pattern))\n    all_dicom_dirs.extend([d for d in found_dirs if d not in all_dicom_dirs])\n\nif all_dicom_dirs:\n    print(f\"üìÅ Found {len(all_dicom_dirs)} DICOM directories\")\n\n    # Process each DICOM directory with metadata extraction\n    for dicom_path in all_dicom_dirs:\n        print(f\"Processing DICOM directory: {dicom_path}\")\n\n        try:\n            # Get all DICOM files in directory\n            dcm_files = []\n            for ext in ['*.dcm', '*.dicom', '*']:\n                dcm_files.extend(list(dicom_path.glob(ext)))\n\n            # Filter actual DICOM files\n            valid_dcm_files = []\n            for dcm_file in dcm_files:\n                try:\n                    ds = pydicom.dcmread(dcm_file, force=True)\n                    if hasattr(ds, 'PatientID'):\n                        valid_dcm_files.append(str(dcm_file))\n                except:\n                    continue\n\n            if valid_dcm_files:\n                # Read DICOM series with SimpleITK\n                reader = sitk.ImageSeriesReader()\n\n                # Get series UIDs\n                series_ids = reader.GetGDCMSeriesIDs(str(dicom_path))\n\n                for series_id in series_ids:\n                    # Get file names for this series\n                    dicom_names = reader.GetGDCMSeriesFileNames(str(dicom_path), series_id)\n\n                    if dicom_names:\n                        reader.SetFileNames(dicom_names)\n\n                        # Extract metadata from first DICOM\n                        sample_ds = pydicom.dcmread(dicom_names[0], force=True)\n\n                        # Extract comprehensive metadata\n                        patient_id = getattr(sample_ds, 'PatientID', 'Unknown')\n                        study_date = getattr(sample_ds, 'StudyDate', 'Unknown')\n                        series_desc = getattr(sample_ds, 'SeriesDescription', 'Unknown')\n                        modality = getattr(sample_ds, 'Modality', 'Unknown')\n                        slice_thickness = getattr(sample_ds, 'SliceThickness', 'Unknown')\n                        pixel_spacing = getattr(sample_ds, 'PixelSpacing', [1.0, 1.0])\n\n                        print(f\"  Series: {series_desc} ({modality})\")\n                        print(f\"  Patient: {patient_id}, Date: {study_date}\")\n                        print(f\"  Slice thickness: {slice_thickness}, Pixel spacing: {pixel_spacing}\")\n\n                        # Read the image\n                        image = reader.Execute()\n\n                        # Convert to proper orientation (RAS)\n                        image = sitk.DICOMOrient(image, 'RAS')\n\n                        # Save as NIfTI with metadata preservation\n                        output_name = f\"{patient_id}_{series_id}_{modality}.nii.gz\"\n                        output_path = dicom_dir / output_name\n\n                        sitk.WriteImage(image, str(output_path))\n\n                        # Save metadata\n                        metadata = {\n                            'patient_id': patient_id,\n                            'study_date': study_date,\n                            'series_description': series_desc,\n                            'modality': modality,\n                            'slice_thickness': str(slice_thickness),\n                            'pixel_spacing': [float(x) for x in pixel_spacing],\n                            'original_dicom_path': str(dicom_path),\n                            'series_id': series_id,\n                            'num_slices': len(dicom_names)\n                        }\n\n                        with open(dicom_dir / f\"{output_name}_metadata.json\", 'w') as f:\n                            json.dump(metadata, f, indent=2)\n\n                        print(f\"  ‚úÖ Converted: {output_name}\")\n\n        except Exception as e:\n            print(f\"  ‚ùå Error processing {dicom_path}: {e}\")\n\n    print(f\"‚úÖ DICOM conversion complete - {len(list(dicom_dir.glob('*.nii.gz')))} files converted\")\nelse:\n    print(\"üìÅ No DICOM directories found - using NIfTI files directly\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:06:30.337931Z","iopub.execute_input":"2025-08-12T11:06:30.338267Z","iopub.status.idle":"2025-08-12T11:06:31.332081Z","shell.execute_reply.started":"2025-08-12T11:06:30.338240Z","shell.execute_reply":"2025-08-12T11:06:31.330813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî¨ nnU-Net Dataset Fingerprinting & Target Spacing Calibration","metadata":{}},{"cell_type":"markdown","source":"## üß† nnU-Net Dataset Fingerprinting","metadata":{}},{"cell_type":"code","source":"print(\"üî¨ nnU-Net DATASET FINGERPRINTING:\")\n\n# Analyze dataset properties for nnU-Net planning\ndataset_properties = {\n    'spacings': [],\n    'shapes': [],\n    'modalities': ['FLAIR', 'T1w', 'T1Gd', 'T2w'],\n    'intensity_properties': {mod: {'percentiles': [], 'mean': [], 'std': []} for mod in\n                             ['FLAIR', 'T1w', 'T1Gd', 'T2w']},\n    'label_properties': {'labels': [], 'volumes': []}\n}\n\n# Extract comprehensive dataset fingerprint\nfor img_path, label_path in tqdm(zip(train_image_files, train_label_files),\n                                 total=len(train_image_files), desc=\"Fingerprinting dataset\"):\n\n    # Load volumes\n    img_nifti = nib.load(img_path)\n    label_nifti = nib.load(label_path)\n\n    img_data = img_nifti.get_fdata()\n    label_data = label_nifti.get_fdata()\n    spacing = img_nifti.header.get_zooms()[:3]\n\n    # Collect spacing and shape information\n    dataset_properties['spacings'].append(spacing)\n    dataset_properties['shapes'].append(img_data.shape[:3])\n\n    # Analyze intensity properties per modality\n    for mod_idx, modality in enumerate(dataset_properties['modalities']):\n        mod_data = img_data[..., mod_idx]\n\n        # Create brain mask using Otsu thresholding\n        sitk_img = sitk.GetImageFromArray(mod_data.astype(np.float32))\n        otsu_filter = sitk.OtsuThresholdImageFilter()\n        otsu_filter.SetInsideValue(0)\n        otsu_filter.SetOutsideValue(1)\n        brain_mask_sitk = otsu_filter.Execute(sitk_img)\n        brain_mask = sitk.GetArrayFromImage(brain_mask_sitk) > 0\n\n        brain_voxels = mod_data[brain_mask]\n\n        if len(brain_voxels) > 1000:  # Ensure sufficient voxels\n            # nnU-Net style percentile analysis\n            percentiles = np.percentile(brain_voxels, [0.5, 10, 50, 90, 99.5])\n            dataset_properties['intensity_properties'][modality]['percentiles'].append(percentiles)\n            dataset_properties['intensity_properties'][modality]['mean'].append(np.mean(brain_voxels))\n            dataset_properties['intensity_properties'][modality]['std'].append(np.std(brain_voxels))\n\n    # Analyze label properties\n    unique_labels = np.unique(label_data)\n    dataset_properties['label_properties']['labels'].append(unique_labels)\n\n    # Calculate label volumes\n    voxel_volume = np.prod(spacing)\n    label_volumes = {}\n    for label_val in unique_labels:\n        if label_val > 0:  # Skip background\n            volume = np.sum(label_data == label_val) * voxel_volume / 1000  # Convert to cm¬≥\n            label_volumes[int(label_val)] = volume\n    dataset_properties['label_properties']['volumes'].append(label_volumes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:06:31.333251Z","iopub.execute_input":"2025-08-12T11:06:31.333610Z","iopub.status.idle":"2025-08-12T11:06:44.008415Z","shell.execute_reply.started":"2025-08-12T11:06:31.333578Z","shell.execute_reply":"2025-08-12T11:06:44.006937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìê Target Spacing & Intensity Normalization Calibration","metadata":{}},{"cell_type":"code","source":"# Calculate nnU-Net target properties\nspacings_array = np.array(dataset_properties['spacings'])\nshapes_array = np.array(dataset_properties['shapes'])\n\n# nnU-Net spacing determination - median spacing\nmedian_spacing = np.median(spacings_array, axis=0)\ntarget_spacing = [float(x) for x in median_spacing]\n\n# Calculate intensity normalization parameters per modality\nnormalization_params = {}\nfor modality in dataset_properties['modalities']:\n    all_percentiles = np.array(dataset_properties['intensity_properties'][modality]['percentiles'])\n    all_means = np.array(dataset_properties['intensity_properties'][modality]['mean'])\n    all_stds = np.array(dataset_properties['intensity_properties'][modality]['std'])\n\n    if len(all_percentiles) > 0:\n        # nnU-Net normalization scheme\n        normalization_params[modality] = {\n            'clip_lower': float(np.median(all_percentiles[:, 0])),  # 0.5th percentile\n            'clip_upper': float(np.median(all_percentiles[:, 4])),  # 99.5th percentile\n            'mean_intensity': float(np.median(all_means)),\n            'std_intensity': float(np.median(all_stds))\n        }\n\nprint(f\"‚úÖ Dataset fingerprinting complete\")\nprint(f\"üìè Target spacing: {target_spacing} mm\")\nprint(f\"üéØ Modalities analyzed: {len(dataset_properties['modalities'])}\")\n\n# Save dataset fingerprint\nfingerprint = {\n    'target_spacing': target_spacing,\n    'normalization_params': normalization_params,\n    'dataset_properties': {\n        'num_cases': len(train_image_files),\n        'modalities': dataset_properties['modalities'],\n        'median_spacing': target_spacing\n    }\n}\n\nwith open(results_dir / 'nnunet_fingerprint.json', 'w') as f:\n    json.dump(fingerprint, f, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:06:44.009366Z","iopub.execute_input":"2025-08-12T11:06:44.009678Z","iopub.status.idle":"2025-08-12T11:06:44.025981Z","shell.execute_reply.started":"2025-08-12T11:06:44.009649Z","shell.execute_reply":"2025-08-12T11:06:44.024369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîß Multi-Site MRI Harmonization Configuration","metadata":{}},{"cell_type":"code","source":"print(\"üîß MULTI-SITE HARMONIZATION PIPELINE:\")\n\n# Harmonization approach for multi-site MRI data\nharmonization_methods = ['n4_bias_correction', 'z_score_harmonization']\n\nprint(f\"Implementing: {', '.join(harmonization_methods)}\")\n\n# Create harmonization reference for z-score standardization\nharmonization_reference = {}\nfor modality in dataset_properties['modalities']:\n    if modality in normalization_params:\n        harmonization_reference[modality] = {\n            'target_mean': 0.0,  # Z-score normalization target\n            'target_std': 1.0,\n            'clip_percentiles': [\n                normalization_params[modality]['clip_lower'],\n                normalization_params[modality]['clip_upper']\n            ]\n        }\n\n# Save harmonization parameters\nharmonization_config = {\n    'methods': harmonization_methods,\n    'harmonization_reference': harmonization_reference,\n    'n4_parameters': {\n        'max_iterations': [50, 50, 30, 20],\n        'convergence_threshold': 1e-6,\n        'bspline_fitting_distance': 300,\n        'shrink_factor': 3\n    }\n}\n\nwith open(results_dir / 'harmonization_config.json', 'w') as f:\n    json.dump(harmonization_config, f, indent=2)\n\nprint(\"‚úÖ Multi-site harmonization parameters configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:06:44.026781Z","iopub.execute_input":"2025-08-12T11:06:44.027076Z","iopub.status.idle":"2025-08-12T11:06:44.058687Z","shell.execute_reply.started":"2025-08-12T11:06:44.027052Z","shell.execute_reply":"2025-08-12T11:06:44.057814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üöÄ nnU-Net v2 Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ nnU-Net v2 PREPROCESSING:\")\nprint(f\"Processing {len(train_image_files)} volumes with harmonization pipeline\")\nprint(\"=\" * 60)\n\nprocessing_stats = []\n\nfor idx, (img_path, label_path) in enumerate(tqdm(zip(train_image_files, train_label_files),\n                                                  desc=\"nnU-Net v2 preprocessing\")):\n\n    case_id = img_path.stem.split('.')[0]\n\n    try:\n        # STEP 1: LOAD VOLUMES AND EXTRACT METADATA\n        img_nifti = nib.load(img_path)\n        label_nifti = nib.load(label_path)\n\n        img_data = img_nifti.get_fdata().astype(np.float32)\n        label_data = label_nifti.get_fdata().astype(np.uint8)\n        original_spacing = img_nifti.header.get_zooms()[:3]\n        original_affine = img_nifti.affine\n\n        # STEP 2: BRAIN EXTRACTION\n        # Use 2% threshold on all non-zero voxels\n        all_voxels = img_data[img_data != 0]\n        low_thresh = np.percentile(all_voxels, 2.0)\n\n        # Create initial mask from FLAIR (modality 0)\n        brain_mask = img_data[..., 0] > low_thresh\n\n        # Apply dilation radius (5x5x5)\n        brain_mask = ndimage.binary_dilation(brain_mask, structure=np.ones((5, 5, 5)))\n\n        # Fill holes\n        brain_mask = ndimage.binary_fill_holes(brain_mask)\n\n        # Keep largest connected component\n        labeled_mask, num_labels = ndimage.label(brain_mask)\n        if num_labels > 1:\n            sizes = ndimage.sum(brain_mask, labeled_mask, range(1, num_labels + 1))\n            largest_label = np.argmax(sizes) + 1\n            brain_mask = labeled_mask == largest_label\n\n        # Calculate coverage using bounding box method\n        bbox = np.argwhere(brain_mask)\n        if len(bbox) > 0:\n            mins, maxs = bbox.min(axis=0), bbox.max(axis=0) + 1\n            bbox_volume = np.prod(maxs - mins)\n            brain_voxels_in_bbox = np.sum(brain_mask[mins[0]:maxs[0], mins[1]:maxs[1], mins[2]:maxs[2]])\n            brain_coverage = (brain_voxels_in_bbox / bbox_volume) * 100\n        else:\n            brain_coverage = 0.0\n\n        # STEP 3: N4 BIAS FIELD CORRECTION\n        img_corrected = np.zeros_like(img_data)\n\n        # Ensure spacing is a Python list of floats\n        spacing = list(map(float, img_nifti.header.get_zooms()[:3]))\n\n        for modality_idx in range(img_data.shape[-1]):\n            mod_data = img_data[..., modality_idx]\n\n            # Convert to SimpleITK images\n            sitk_img_mod = sitk.GetImageFromArray(mod_data.astype(np.float32))\n            sitk_mask_mod = sitk.GetImageFromArray(brain_mask.astype(np.uint8))\n\n            # Preserve original NIfTI spacing/orientation\n            sitk_img_mod.SetSpacing(spacing)\n            sitk_mask_mod.SetSpacing(spacing)\n\n            # Shrink for computational efficiency\n            shrink_factor = 4\n            img_shrunk = sitk.Shrink(sitk_img_mod, [shrink_factor] * 3)\n            mask_shrunk = sitk.Shrink(sitk_mask_mod, [shrink_factor] * 3)\n\n            # N4 correction\n            corrector = sitk.N4BiasFieldCorrectionImageFilter()\n            corrector.SetMaximumNumberOfIterations([50, 50, 30, 20])\n            corrector.SetConvergenceThreshold(1e-6)\n\n            try:\n                corrected_shrunk = corrector.Execute(img_shrunk, mask_shrunk)\n\n                # Get bias field and apply to full resolution\n                log_bias_field = corrector.GetLogBiasFieldAsImage(sitk_img_mod)\n                bias_field = np.exp(sitk.GetArrayFromImage(log_bias_field))\n\n                corrected_fullres = mod_data / (bias_field + 1e-8)\n                img_corrected[..., modality_idx] = corrected_fullres.astype(np.float32)\n\n            except Exception as e:\n                print(f\"    ‚ö†Ô∏è N4 failed for modality {modality_idx}: {e}\")\n                img_corrected[..., modality_idx] = mod_data\n\n        # STEP 4: nnU-Net INTENSITY NORMALIZATION\n        img_normalized = np.zeros_like(img_corrected)\n\n        for modality_idx, modality in enumerate(dataset_properties['modalities']):\n            mod_data = img_corrected[..., modality_idx]\n\n            if modality not in normalization_params:\n                img_normalized[..., modality_idx] = mod_data * brain_mask\n                continue\n\n            # nnU-Net style percentile clipping on full image\n            clip_lower = normalization_params[modality]['clip_lower']\n            clip_upper = normalization_params[modality]['clip_upper']\n\n            mod_data_clipped = np.clip(mod_data, clip_lower, clip_upper)\n\n            # Z-score normalization using global parameters\n            brain_voxels_clipped = mod_data_clipped[brain_mask]\n            if len(brain_voxels_clipped) > 0:\n                mean_val = normalization_params[modality]['mean_intensity']\n                std_val = normalization_params[modality]['std_intensity']\n\n                if std_val > 0:\n                    # Normalize full image, then apply brain mask\n                    normalized_full = (mod_data_clipped - mean_val) / std_val\n                    img_normalized[..., modality_idx] = normalized_full * brain_mask\n                else:\n                    img_normalized[..., modality_idx] = (mod_data_clipped - mean_val) * brain_mask\n            else:\n                img_normalized[..., modality_idx] = mod_data_clipped * brain_mask\n\n        # STEP 5: SPATIAL RESAMPLING TO nnU-Net TARGET\n        zoom_factors = [orig / target for orig, target in zip(original_spacing, target_spacing)]\n\n        # Calculate new shape after resampling\n        new_shape = [int(dim * zoom) for dim, zoom in zip(img_normalized.shape[:3], zoom_factors)]\n\n        # Resample images with cubic interpolation\n        img_resampled = np.zeros((*new_shape, img_normalized.shape[-1]), dtype=np.float32)\n        for modality_idx in range(img_normalized.shape[-1]):\n            img_resampled[..., modality_idx] = ndimage.zoom(\n                img_normalized[..., modality_idx], zoom_factors, order=3\n            )\n\n        # Resample labels with nearest neighbor\n        label_resampled = ndimage.zoom(label_data, zoom_factors, order=0).astype(np.uint8)\n\n        # Resample brain mask\n        brain_mask_resampled = ndimage.zoom(\n            brain_mask.astype(np.uint8), zoom_factors, order=0\n        ).astype(bool)\n\n        # Recalculate bounding box for resampled brain mask\n        bbox_resampled = np.argwhere(brain_mask_resampled)\n        if len(bbox_resampled) > 0:\n            mins_resampled, maxs_resampled = bbox_resampled.min(axis=0), bbox_resampled.max(axis=0) + 1\n        else:\n            mins_resampled, maxs_resampled = [0, 0, 0], brain_mask_resampled.shape\n\n        # STEP 6: QUALITY CONTROL AND VALIDATION\n        # Handle NaN/Inf values\n        if np.any(np.isnan(img_resampled)) or np.any(np.isinf(img_resampled)):\n            print(\"    ‚ö†Ô∏è Found NaN/Inf values - applying correction...\")\n            img_resampled = np.nan_to_num(img_resampled, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Validate label integrity\n        valid_labels = [0, 1, 2, 3]  # Background, Edema, Non-enhancing, Enhancing\n        invalid_voxels = ~np.isin(label_resampled, valid_labels)\n        if np.any(invalid_voxels):\n            print(f\"    ‚ö†Ô∏è Found {np.sum(invalid_voxels)} invalid label voxels - setting to background\")\n            label_resampled = np.where(np.isin(label_resampled, valid_labels), label_resampled, 0)\n\n        # Check tumor preservation\n        tumor_before = np.sum(label_data > 0)\n        tumor_after = np.sum(label_resampled > 0)\n        preservation_ratio = tumor_after / (tumor_before + 1e-8)\n        print(f\"    üìä Tumor preservation: {preservation_ratio:.3f} ({tumor_before} ‚Üí {tumor_after} voxels)\")\n\n        # STEP 7: SAVE nnU-Net FORMAT AND METADATA\n        # Create nnU-Net file paths\n        output_img_path = nnunet_dir / 'imagesTr' / f\"{case_id}_0000.nii.gz\"\n        output_label_path = nnunet_dir / 'labelsTr' / f\"{case_id}.nii.gz\"\n\n        # Update affine matrix for new spacing\n        new_affine = original_affine.copy()\n        new_affine[0, 0] = target_spacing[0] if new_affine[0, 0] > 0 else -target_spacing[0]\n        new_affine[1, 1] = target_spacing[1] if new_affine[1, 1] > 0 else -target_spacing[1]\n        new_affine[2, 2] = target_spacing[2] if new_affine[2, 2] > 0 else -target_spacing[2]\n\n        # Save NIfTI files\n        img_nifti_out = nib.Nifti1Image(img_resampled.astype(np.float32), new_affine)\n        label_nifti_out = nib.Nifti1Image(label_resampled.astype(np.uint8), new_affine)\n\n        nib.save(img_nifti_out, output_img_path)\n        nib.save(label_nifti_out, output_label_path)\n\n        # Save preprocessing metadata\n        np.savez_compressed(\n            processed_dir / f\"{case_id}_preprocessed.npz\",\n            image=img_resampled.astype(np.float32),\n            label=label_resampled.astype(np.uint8),\n            brain_mask_full=brain_mask_resampled,\n            brain_mask_roi=brain_mask_resampled[mins_resampled[0]:maxs_resampled[0],\n                           mins_resampled[1]:maxs_resampled[1],\n                           mins_resampled[2]:maxs_resampled[2]],\n            brain_mask_bbox_mins=mins_resampled,\n            brain_mask_bbox_maxs=maxs_resampled,\n            original_spacing=original_spacing,\n            target_spacing=target_spacing,\n            original_shape=img_data.shape[:3],\n            final_shape=img_resampled.shape[:3],\n            normalization_applied=True,\n            harmonization_applied=True,\n            bias_correction_applied=True,\n            histogram_matching_applied=False\n        )\n\n        # Record processing statistics\n        processing_stats.append({\n            'case_id': case_id,\n            'success': True,\n            'original_shape': img_data.shape[:3],\n            'final_shape': img_resampled.shape[:3],\n            'original_spacing': original_spacing,\n            'final_spacing': target_spacing,\n            'tumor_voxels_before': int(tumor_before),\n            'tumor_voxels_after': int(tumor_after),\n            'tumor_preservation_ratio': float(preservation_ratio),\n            'brain_mask_coverage': float(brain_coverage),\n            'intensity_range_per_modality': [\n                [float(img_resampled[..., i].min()), float(img_resampled[..., i].max())]\n                for i in range(img_resampled.shape[-1])\n            ]\n        })\n\n        print(f\"  ‚úÖ {case_id} processing completed successfully!\")\n\n        # Progress update\n        if (idx + 1) % 2 == 0 or (idx + 1) == len(train_image_files):\n            successful = sum(1 for s in processing_stats if s['success'])\n            print(f\"\\nüìä Progress: {idx + 1}/{len(train_image_files)} cases processed ({successful} successful)\")\n    except Exception as e:\n        print(f\"  ‚ùå Error processing {case_id}: {e}\")\n        processing_stats.append({\n            'case_id': case_id,\n            'success': False,\n            'error': str(e)\n        })\n\n# Save comprehensive processing statistics\nprocessing_df = pd.DataFrame(processing_stats)\nprocessing_df.to_csv(results_dir / 'nnunet_preprocessing_stats.csv', index=False)\n\nsuccessful_cases = processing_df['success'].sum()\nfailed_cases = len(processing_df) - successful_cases\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üéØ nnU-Net v2 PREPROCESSING COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"‚úÖ Successfully processed: {successful_cases}/{len(processing_df)} volumes\")\nprint(f\"‚ùå Failed cases: {failed_cases}\")\nprint(f\"üìä Success rate: {successful_cases / len(processing_df) * 100:.1f}%\")\n\nif successful_cases > 0:\n    successful_stats = processing_df[processing_df['success']].copy()\n    avg_preservation = successful_stats['tumor_preservation_ratio'].mean()\n    avg_brain_coverage = successful_stats['brain_mask_coverage'].mean()\n\n    print(f\"üìä Average tumor preservation: {avg_preservation:.3f}\")\n    print(f\"üß† Average brain mask coverage: {avg_brain_coverage:.1f}%\")\n\nprint(f\"\\nüìÅ Outputs saved to:\")\nprint(f\"  ‚Ä¢ nnU-Net format: {nnunet_dir}\")\nprint(f\"  ‚Ä¢ Preprocessed data: {processed_dir}\")\nprint(f\"  ‚Ä¢ Statistics: {results_dir}\")\nprint(\"\\nüöÄ Ready for nnU-Net training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:06:44.060238Z","iopub.execute_input":"2025-08-12T11:06:44.060640Z","iopub.status.idle":"2025-08-12T11:11:37.748574Z","shell.execute_reply.started":"2025-08-12T11:06:44.060596Z","shell.execute_reply":"2025-08-12T11:11:37.747408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìú nnU-Net Dataset JSON & Cross-Validation Setup","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport json\n\n# Convert to JSON-safe format\nnnunet_dataset_json = {\n    \"channel_names\": {\n        \"0\": \"FLAIR\",\n        \"1\": \"T1w\",\n        \"2\": \"T1Gd\",\n        \"3\": \"T2w\"\n    },\n    \"labels\": {\n        \"background\": 0,\n        \"edema\": 1,\n        \"non_enhancing_tumor\": 2,\n        \"enhancing_tumor\": 3\n    },\n    \"regions_class_order\": [1, 2, 3],\n    \"numTraining\": int(successful_cases),\n    \"file_ending\": \".nii.gz\",\n    \"overwrite_image_reader_writer\": \"NibabelIOWithReorient\",\n    \"nnUNet_version\": \"2.0\",\n    \"dataset_name\": \"Dataset001_BrainTumor\",\n    \"description\": \"Brain tumor segmentation with nnU-Net v2 preprocessing\",\n    \"reference\": \"BraTS challenge targeting WT Dice ‚â• 90%, BraTS Avg ‚â• 80%\",\n    \"tensorImageSize\": \"4D\",\n    \"training\": [{\"image\": f\"./imagesTr/{str(case['case_id'])}_0000.nii.gz\",\n                  \"label\": f\"./labelsTr/{str(case['case_id'])}.nii.gz\"}\n                 for case in processing_stats if case['success']]\n}\n\nwith open(nnunet_dir / 'dataset.json', 'w') as f:\n    json.dump(nnunet_dataset_json, f, indent=2)\n\n# Extract successful case IDs\nall_cases = [str(stats['case_id']) for stats in processing_stats if stats['success']]\n\n# Create splits if more than 1 case\nsplits = []\nif len(all_cases) > 1:\n    kfold = KFold(n_splits=min(5, len(all_cases)), shuffle=True, random_state=42)\n    for train_idx, val_idx in kfold.split(all_cases):\n        train_cases = [all_cases[i] for i in train_idx]\n        val_cases = [all_cases[i] for i in val_idx]\n        splits.append({\n            \"train\": train_cases,\n            \"val\": val_cases\n        })\nelse:\n    splits = [{\"train\": all_cases, \"val\": all_cases}]\n\nwith open(nnunet_dir / 'splits_final.json', 'w') as f:\n    json.dump(splits, f, indent=2)\n\n# Print result summary\nprint(f\"‚úÖ nnU-Net dataset configuration complete\")\nprint(f\"üìÅ Training images: {len(list((nnunet_dir / 'imagesTr').glob('*.nii.gz')))} files\")\nprint(f\"üìÅ Training labels: {len(list((nnunet_dir / 'labelsTr').glob('*.nii.gz')))} files\")\nprint(f\"üìÅ Cross-validation: {len(splits)} folds configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:11:37.749641Z","iopub.execute_input":"2025-08-12T11:11:37.749978Z","iopub.status.idle":"2025-08-12T11:11:37.766077Z","shell.execute_reply.started":"2025-08-12T11:11:37.749947Z","shell.execute_reply":"2025-08-12T11:11:37.764998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ü©∫ Clinical Volume Metrics & Image Quality Analysis","metadata":{}},{"cell_type":"code","source":"# Load preprocessed data for clinical analysis\nmetadata_list = []\n\nfor img_path, label_path in zip(train_image_files, train_label_files):\n    case_id = img_path.stem.split('.')[0]\n\n    # Load original data for clinical metrics\n    img_nifti = nib.load(img_path)\n    label_nifti = nib.load(label_path)\n\n    img_data = img_nifti.get_fdata()\n    label_data = label_nifti.get_fdata()\n    spacing = img_nifti.header.get_zooms()[:3]\n\n    # Calculate BraTS standard metrics\n    voxel_volume_cm3 = np.prod(spacing) / 1000\n\n    # BraTS tumor regions\n    whole_tumor = label_data > 0\n    tumor_core = (label_data == 2) | (label_data == 3)\n    enhancing_tumor = label_data == 3\n\n    wt_volume = np.sum(whole_tumor) * voxel_volume_cm3\n    tc_volume = np.sum(tumor_core) * voxel_volume_cm3\n    et_volume = np.sum(enhancing_tumor) * voxel_volume_cm3\n\n    # Image quality metrics\n    modality_stats = {}\n    for mod_idx, modality in enumerate(['FLAIR', 'T1w', 'T1Gd', 'T2w']):\n        mod_data = img_data[..., mod_idx]\n        brain_mask = mod_data > np.percentile(mod_data[mod_data > 0], 5)\n        brain_voxels = mod_data[brain_mask]\n\n        if len(brain_voxels) > 0:\n            modality_stats[f'{modality}_snr'] = np.mean(brain_voxels) / (np.std(brain_voxels) + 1e-8)\n            modality_stats[f'{modality}_mean'] = np.mean(brain_voxels)\n            modality_stats[f'{modality}_std'] = np.std(brain_voxels)\n        else:\n            modality_stats[f'{modality}_snr'] = 0\n            modality_stats[f'{modality}_mean'] = 0\n            modality_stats[f'{modality}_std'] = 0\n\n    metadata_list.append({\n        'case_id': case_id,\n        'wt_volume_cm3': wt_volume,\n        'tc_volume_cm3': tc_volume,\n        'et_volume_cm3': et_volume,\n        'tumor_present': wt_volume > 0,\n        'original_spacing': spacing,\n        **modality_stats\n    })\n\n# Create clinical metadata DataFrame\nclinical_df = pd.DataFrame(metadata_list)\nclinical_df.to_csv(results_dir / 'clinical_analysis.csv', index=False)\n\n# Clinical visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# BraTS volume distributions\nvolume_cols = ['wt_volume_cm3', 'tc_volume_cm3', 'et_volume_cm3']\ncolors = ['royalblue', 'crimson', 'forestgreen']\ntitles = ['Whole Tumor (WT)', 'Tumor Core (TC)', 'Enhancing Tumor (ET)']\n\nfor idx, (col, color, title) in enumerate(zip(volume_cols, colors, titles)):\n    ax = axes[0, idx]\n    data_to_plot = clinical_df[clinical_df[col] > 0][col]\n    if len(data_to_plot) > 0:\n        data_to_plot.hist(bins=10, alpha=0.7, ax=ax, color=color, edgecolor='black')\n    ax.set_title(f'{title} Volume Distribution')\n    ax.set_xlabel('Volume (cm¬≥)')\n    ax.set_ylabel('Frequency')\n    ax.grid(True, alpha=0.3)\n\n# Image quality metrics\nquality_cols = ['FLAIR_snr', 'T1w_snr', 'T1Gd_snr']\nfor idx, col in enumerate(quality_cols):\n    ax = axes[1, idx]\n    clinical_df[col].hist(bins=10, alpha=0.7, ax=ax, color='orange', edgecolor='black')\n    ax.set_title(f'{col.replace(\"_\", \" \").upper()}')\n    ax.set_xlabel('Signal-to-Noise Ratio')\n    ax.set_ylabel('Frequency')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(visualization_dir / 'clinical_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Handle single-case stats display\nif len(clinical_df) == 1:\n    wt_std = tc_std = et_std = \"N/A\"\nelse:\n    wt_std = f\"{clinical_df['wt_volume_cm3'].std():.1f}\"\n    tc_std = f\"{clinical_df['tc_volume_cm3'].std():.1f}\"\n    et_std = f\"{clinical_df['et_volume_cm3'].std():.1f}\"\n\nprint(\"üìä Clinical analysis complete\")\nprint(f\"‚Ä¢ Cases with tumor: {clinical_df['tumor_present'].sum()}/{len(clinical_df)}\")\nprint(f\"‚Ä¢ Mean WT volume: {clinical_df['wt_volume_cm3'].mean():.1f} ¬± {wt_std} cm¬≥\")\nprint(f\"‚Ä¢ Mean TC volume: {clinical_df['tc_volume_cm3'].mean():.1f} ¬± {tc_std} cm¬≥\")\nprint(f\"‚Ä¢ Mean ET volume: {clinical_df['et_volume_cm3'].mean():.1f} ¬± {et_std} cm¬≥\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:11:37.769007Z","iopub.execute_input":"2025-08-12T11:11:37.769324Z","iopub.status.idle":"2025-08-12T11:11:44.440788Z","shell.execute_reply.started":"2025-08-12T11:11:37.769290Z","shell.execute_reply":"2025-08-12T11:11:44.439475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç Preprocessing Validation & Visual Quality Control","metadata":{}},{"cell_type":"code","source":"# Load sample preprocessed data for validation\nsample_files = list(processed_dir.glob('*_preprocessed.npz'))\n\nif sample_files:\n    print(\"üîç PREPROCESSING VALIDATION:\")\n\n    # Validate first sample\n    sample_data = np.load(sample_files[0])\n    print(f\"Sample case: {sample_files[0].stem}\")\n    print(f\"Image shape: {sample_data['image'].shape}\")\n    print(f\"Label shape: {sample_data['label'].shape}\")\n    print(f\"Target spacing: {sample_data['target_spacing']}\")\n    print(f\"Unique labels: {np.unique(sample_data['label'])}\")\n    print(f\"Image intensity range per modality:\")\n    for i in range(sample_data['image'].shape[-1]):\n        mod_min = sample_data['image'][..., i].min()\n        mod_max = sample_data['image'][..., i].max()\n        mod_mean = np.mean(sample_data['image'][..., i][sample_data['image'][..., i] != 0])\n        print(f\"  Modality {i}: [{mod_min:.3f}, {mod_max:.3f}], mean: {mod_mean:.3f}\")\n\n    # ROI brain mask coverage\n    if 'brain_mask_roi' in sample_data:\n        roi_mask = sample_data['brain_mask_roi']\n        roi_coverage = np.sum(roi_mask) / np.prod(roi_mask.shape) * 100\n        print(f\"Brain mask coverage (ROI): {roi_coverage:.1f}%\")\n    else:\n        print(\"‚ö†Ô∏è No ROI mask found in file\")\n\n    print(f\"Normalization applied: {sample_data['normalization_applied']}\")\n    print(f\"Harmonization applied: {sample_data['harmonization_applied']}\")\n\n# Visualization\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\n\n# Load first three cases for comprehensive validation\nvis_cases = sample_files[:min(3, len(sample_files))]\n\nfor row, case_file in enumerate(vis_cases):\n    data = np.load(case_file)\n    image = data['image']\n    label = data['label']\n\n    # Use full brain mask\n    brain_mask_full = data['brain_mask_full'] if 'brain_mask_full' in data else None\n\n    mid_slice = image.shape[2] // 2\n    modality_names = ['FLAIR', 'T1w', 'T1Gd', 'T2w']\n\n    for col, modality in enumerate(modality_names):\n        ax = axes[row, col]\n\n        # Display normalized image\n        ax.imshow(image[:, :, mid_slice, col], cmap='gray', vmin=-3, vmax=3)\n\n        # Overlay segmentation mask\n        if np.any(label[:, :, mid_slice] > 0):\n            overlay = np.zeros((*label[:, :, mid_slice].shape, 4))\n            overlay[label[:, :, mid_slice] == 1] = [0, 1, 0, 0.4]  # Edema - green\n            overlay[label[:, :, mid_slice] == 2] = [1, 1, 0, 0.4]  # Non-enhancing - yellow\n            overlay[label[:, :, mid_slice] == 3] = [1, 0, 0, 0.4]  # Enhancing - red\n            ax.imshow(overlay)\n\n        # Use full brain mask for contour\n        if brain_mask_full is not None:\n            mask_slice = brain_mask_full[:, :, mid_slice]\n            ax.contour(mask_slice, colors='cyan', linewidths=0.5)\n\n        ax.set_title(f'{case_file.stem.split(\"_\")[0]} - {modality}')\n        ax.axis('off')\n\nplt.suptitle('nnU-Net Preprocessed Volumes - Quality Control', fontsize=16)\nplt.tight_layout()\nplt.savefig(visualization_dir / 'preprocessing_validation.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:11:44.441993Z","iopub.execute_input":"2025-08-12T11:11:44.442271Z","iopub.status.idle":"2025-08-12T11:12:11.904220Z","shell.execute_reply.started":"2025-08-12T11:11:44.442248Z","shell.execute_reply":"2025-08-12T11:12:11.902512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìë Final Preprocessing Report","metadata":{}},{"cell_type":"code","source":"# Generate comprehensive preprocessing report\nfinal_report = {\n    'preprocessing_summary': {\n        'total_cases_input': int(len(train_image_files)),\n        'successfully_processed': int(successful_cases),\n        'success_rate_percent': round(float(successful_cases / len(train_image_files) * 100), 1),\n        'target_metrics': 'WT Dice ‚â• 90%, BraTS Avg ‚â• 80%'\n    },\n    'nnunet_configuration': {\n        'dataset_name': 'Dataset001_BrainTumor',\n        'target_spacing_mm': [float(x) for x in target_spacing],\n        'modalities': ['FLAIR', 'T1w', 'T1Gd', 'T2w'],\n        'labels': ['background', 'edema', 'non_enhancing_tumor', 'enhancing_tumor'],\n        'cross_validation_folds': int(len(splits))\n    },\n    'clinical_characteristics': {\n        'cases_with_tumor': int(clinical_df['tumor_present'].sum()),\n        'mean_wt_volume_cm3': round(float(clinical_df['wt_volume_cm3'].mean()), 2),\n        'mean_tc_volume_cm3': round(float(clinical_df['tc_volume_cm3'].mean()), 2),\n        'mean_et_volume_cm3': round(float(clinical_df['et_volume_cm3'].mean()), 2)\n    },\n    'preprocessing_pipeline': [\n        'Brain extraction using 2% threshold + 5x5x5 dilation',\n        'N4 bias field correction for multi-site harmonization',\n        'nnU-Net v2 intensity normalization with global parameters',\n        'Spacing-based resampling with cubic interpolation',\n        'Quality control with NaN/Inf handling and label validation'\n    ],\n    'quality_metrics': {\n        'mean_brain_mask_coverage_percent': round(float(np.mean([\n            s['brain_mask_coverage'] for s in processing_stats if s['success']\n        ])), 1),\n        'mean_tumor_preservation_ratio': round(float(np.mean([\n            s['tumor_preservation_ratio'] for s in processing_stats if s['success']\n        ])), 3)\n    }\n}\n\n# Save report\nwith open(results_dir / 'final_preprocessing_report.json', 'w') as f:\n    json.dump(final_report, f, indent=2)\n\n# Handle single-case stats\nwt_std = clinical_df['wt_volume_cm3'].std() if len(clinical_df) > 1 else 0\ntc_std = clinical_df['tc_volume_cm3'].std() if len(clinical_df) > 1 else 0\net_std = clinical_df['et_volume_cm3'].std() if len(clinical_df) > 1 else 0\n\n# Final summary\nprint(\"üéØ nnU-Net v2 PREPROCESSING COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"‚úÖ Successfully processed: {successful_cases}/{len(train_image_files)} volumes\")\nprint(f\"‚úÖ Brain mask coverage: {np.mean([s['brain_mask_coverage'] for s in processing_stats if s['success']]):.1f}%\")\nprint(f\"‚úÖ Tumor preservation: {np.mean([s['tumor_preservation_ratio'] for s in processing_stats if s['success']]):.3f}\")\n\nprint(f\"\\nüìä CLINICAL METRICS:\")\nprint(f\"‚Ä¢ Whole Tumor: {clinical_df['wt_volume_cm3'].mean():.1f} ¬± {wt_std:.1f} cm¬≥\")\nprint(f\"‚Ä¢ Tumor Core: {clinical_df['tc_volume_cm3'].mean():.1f} ¬± {tc_std:.1f} cm¬≥\")\nprint(f\"‚Ä¢ Enhancing Tumor: {clinical_df['et_volume_cm3'].mean():.1f} ¬± {et_std:.1f} cm¬≥\")\n\nprint(f\"\\nüöÄ READY FOR: BRAIN_model_training.ipynb\")\n\n# Memory cleanup\nimport gc\ngc.collect()\nprint(\"‚úÖ Memory cleaned - preprocessing pipeline complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:12:11.905641Z","iopub.execute_input":"2025-08-12T11:12:11.906012Z","iopub.status.idle":"2025-08-12T11:12:12.076831Z","shell.execute_reply.started":"2025-08-12T11:12:11.905982Z","shell.execute_reply":"2025-08-12T11:12:12.075965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip freeze","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:12:12.077674Z","iopub.execute_input":"2025-08-12T11:12:12.078028Z","iopub.status.idle":"2025-08-12T11:12:12.105770Z","shell.execute_reply.started":"2025-08-12T11:12:12.078004Z","shell.execute_reply":"2025-08-12T11:12:12.104830Z"},"scrolled":true},"outputs":[],"execution_count":null}]}